# -*- coding: utf-8 -*-
"""UAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O-Ya1LKKz_RJ5wDkFnPwd-01xg6tPNep
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/MyDrive/ANDAT/UAS/"

train = pd.read_excel(path + "train.xlsx")
test  = pd.read_excel(path + "test.xlsx")

train.head(), test.head()

train.shape
train.info()
train.head()

train["Provinsi Faskes"].nunique()

#Fitur 1 (Nentuin jumlah faskes dari ID kunjungan FKTP)
jumlah_faskes = (
    train
    .groupby("Provinsi Faskes")["ID Kunjungan FKTP"]
    .nunique()
    .reset_index(name="jumlah_faskes")
)

jumlah_faskes.head()

jumlah_faskes.shape

#Fitur 2 (Jumlah Kunjungan)
jumlah_kunjungan = (
    train
    .groupby("Provinsi Faskes")
    .size()
    .reset_index(name="jumlah_kunjungan")
)

jumlah_kunjungan.head()

jumlah_kunjungan.shape

#Gabungin Fitur 1 dan 2 biar tiap baris provinsi mencakup 2 fitur tersebut
fitur = jumlah_faskes.merge(
    jumlah_kunjungan,
    on="Provinsi Faskes"
)

fitur.head()

fitur.shape

#Fitur 3 (Rata-rata Beban layanan faskes tiap provinsi)
rata_bobot = (
    train
    .groupby("Provinsi Faskes")["Bobot"]
    .mean()
    .reset_index(name="rata_bobot")
)
rata_bobot.head()

rata_bobot.shape

#Gabungin ke fitur yang uda ada
fitur = fitur.merge(
    rata_bobot,
    on="Provinsi Faskes"
)

fitur.head()

fitur.shape

#Fitur 4 (Total beban keseluruhan wilayah)
total_bobot = (
    train
    .groupby("Provinsi Faskes")["Bobot"]
    .sum()
    .reset_index(name="total_bobot")
)
total_bobot.head()

total_bobot.shape

#Gabungin fitur
fitur = fitur.merge(
    total_bobot,
    on="Provinsi Faskes"
)

fitur.head()

#Cek final bentuk fitur
fitur.shape

#Pisahin kolom provinsi dan angka
X = fitur.drop(columns=["Provinsi Faskes"])
X.head()

from sklearn.preprocessing import StandardScaler
#Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Elbow Method
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K = range(1, 8)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(K, inertia, marker="o")
plt.xlabel("Jumlah Cluster")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

#Train Models
kmeans = KMeans(n_clusters=3, random_state=42)
cluster = kmeans.fit_predict(X_scaled)

#Tambahin Hasil Cluster ke data
fitur["cluster"] = cluster
fitur.head()

fitur.groupby("cluster")[[
    "jumlah_faskes",
    "jumlah_kunjungan",
    "rata_bobot",
    "total_bobot"
]].mean()

label_map = {
    0: "Kurang Memadai",
    2: "Cukup Memadai",
    1: "Memadai"
}

fitur["kategori_faskes"] = fitur["cluster"].map(label_map)
fitur[["Provinsi Faskes", "cluster", "kategori_faskes"]].head()

fitur["kategori_faskes"].value_counts()

#cek interval validation (Apakah cluster yang terbentuk itu rapih, terpisah, dan masuk akal?)
from sklearn.metrics import silhouette_score

sil_score = silhouette_score(X_scaled, cluster)
sil_score

scores = {}

for k in range(2, 7):
    km = KMeans(n_clusters=k, random_state=42)
    lbl = km.fit_predict(X_scaled)
    scores[k] = silhouette_score(X_scaled, lbl)

scores

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            c='black', marker='X', s=200)
plt.show()

import pickle

with open("model_clustering.pkl", "wb") as f:
    pickle.dump(kmeans, f)

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

with open("model_clustering.ipynb", "wb") as f:
    pickle.dump(kmeans, f)